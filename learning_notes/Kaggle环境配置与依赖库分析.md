- .
    ```
    !pip install -q transformers datasets accelerate sentencepiece peft bitsandbytes trl evaluate
    ```
- 
- transformers完成了什么工作 >>>
    - 屏蔽模型层面的差异，统一对外的接口
    - 调用不同模型时，操作差不多
- 
- 模型的本质是什么，是程序还是参数 >>>
    - 模型可以理解为广义上的参数，而不是程序，因为模型不能接受输入产生输出
    - 根据模型的定义来改造pytorch，最终是pytorch完成计算并输出结果
    - 从模型文件角度分析，模型的本质是数据而非程序
    - 模型向计算框架描述了网络结构的特点和参数的值
- 
- pytorch完成什么工作，如何理解transformers库与pytorch的关系 >>>
    - pytorch接受与模型相关的数据，生成一个可执行的计算图对象
    - transformers中调用了pytroch 接口，而不是直接内置一个pytorch
- 
- 解释一个模型的结构，什么唯一确定了一个模型 >>>
    - 模型包含模型结构文件和预训练权重
    - 模型结构是宏观设计，通常在训练之前就确定好，主要关注层数，神经元个数，注意力头数等基础信息
    - 确定好宏观设计，就可以用数据集开始训练，训练过程中宏观设计总体不变，但是细节处改变，预训练权重记录了这些改变的细节
- 
- 模型结构更准确的说法是模型架构
- 常见情况下，模型结构不会在训练中改变，改变的是内部的细节
- 
- 
- 
- tokenizer是什么，完成了什么工作 >>>
    - 分词器，内置分词算法
    - tokenizer将人类可读的语言转化为模型可处理的token，进而转化为token id
    - 最终将模型输出的token id转化为token，最后转化为自然语言
    - 编码器加解码器
- 
- token id的个数是否固定 >>>
    - 是固定的
    - 给定tokenizer，输入一个单词，会得到一个id组，内含多个id
    - 但不会得到意料之外的id
- 
- tokenizer是否和模型绑定，为什么 >>>
    - 和模型绑定
    - 因为需要保证在任何时候，将自然语言翻译成模型语言的翻译器是完全相同的
    - 模型的训练依赖唯一的token id编码方式
- 
- tokenizer，token id 与模型绑定。
- 
- 考量同一类模型下两个不同的模型的模型架构，其构成材料是否相同 >>>
    - 是相同的
    - 基本上都有前馈层，归一化层等
    - 只是组织方式，结构不同
    - 注意前提要求在同一类模型下
- 
- token id与模型的哪一部分绑定 >>>
    - 与模型的embedding层绑定，不影响模型的整体架构
- 
- transformers库中的AutoModelForCausalLM, AutoTokenizer是如何工作的，简单描述 >>>
    - 拿到输入的模型名称
    - 与hugging face的仓库交互，拿到对应的模型架构文件，预训练权重，和tokenizer配置文件
- 
- 
- 
- Datasets库完成了什么工作，简要解释 >>>
    - 完成了载入数据库，并将数据库中的数据处理成为适合计算图使用的token id
- 
- 为什么在做数据格式化时需要统一转化为prompt和response结构 >>>
    - 这样的结构最有利于模型学习理解，解决了instruction和input分离的问题
    - instruction，input，output格式是对人类友好的格式，但是对模型训练不友好
- 
- 如果是多轮对话的数据集，在格式化时需要如何改造 >>>
    - 需要将历史的对话拼接到最新的prompt中，来实现记忆的效果
    - 最终还是改造成prompt和response结构
- 
- 观察多轮对话数据集对应的格式化改造，如何简单归纳 >>>
    - 一个多轮对话组可以拆分成多条独立的prompt response对，样本
    - 这些样本有递增的特点
- 
- 数据格式化是否会抹除样本中的原始字段 >>>
    - 不会
    - 格式化改造只是追加，仍然保留原始字段
- 
- ^^^**追加发生在数据集在内存中的副本，而不是数据集本身**^^^ 
- 
- 为什么在格式化时，仍然要保留原始字段 >>>
    - 相当于保留原材料
    - 如果后期想从原材料出发换一种形式弄出新材料，比较方便
    - 方便处理与调试
- 
- 在tokenize阶段，原始字段是否会参与 >>>
    - 不会参与
    - 我们会指定只有prompt和response对应的字段可以参与
- 
- 用简单的语言描述tokenize步骤 >>>
    - 将格式化之后的样本转化成token id的序列，方便模型的进一步使用
- 
- `dataset.map()` ^^^是对每条样本批量处理的操作，默认不改变原对象，但通过赋值回同名变量可以实现“看起来像原地修改”的效果。  ^^^ 
- 
- 
- 
- peft库
- 
- .
    ```
    from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"], # 针对注意力模块低秩调整
    lora_dropout=0.1, bias="none", task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

    ```
- 
- 解释上面的代码
- 首先从peft库中引入loraconfig和get peft model两个接口，然后用LoraConfig函数构建出一个lora配置参数对象，包含所需的参数
- 最后再用get peft model，先将原始模型权重冻结，后在指定位置注入LoRA层，最后得到一个LoRA改版模型
- 
- 为什么LoRA微调选择低秩矩阵，秩的高低代表着什么 >>>
    - 秩的高低代表信息容量
    - 适应新任务时，往往只需要对原始矩阵做很小的改动
    - 这些小改动又可以很好的被低秩矩阵近似表示
- 
- LoRA与全参数微调的区别是什么 >>>
    - 参数更新的范围不同，LoRA微调范围小，全参数范围大
    - LoRA通过插入低秩矩阵的方式来实现打补丁的效果，只训练少量参数
    - 一般而言，LoRA效率更高
- 
- LoraConfig中的r参数的含义，及对应的作用 >>>
    - rank，矩阵的秩
    - 决定新增LoRA矩阵的容量
    - 容量大，可以表达更加复杂的调整，参数量增加
- 
- Lora alpha解释 >>>
    - 缩放系数，控制lora更新在模型中的权重
    - LoRA矩阵影响力的缩放系数
- 
- LoraConfig中的目标模块参数解释 >>>
    - LoRA矩阵注入的目标模块
    - 注入不同的模块，产生的效果与显存开销不一样
- 
- lora dropout是什么 >>>
    - lora层随机屏蔽机制
    - 引入噪声，防止过拟合
- 
- bias通常选none，task type选CAUSAL LM。
- 
- LoraConfig中的参数决定了什么 >>>
    - 定义了补丁本身的结构和行为
- 
