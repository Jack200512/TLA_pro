- 实验初期使用的模型为 `tiiuae/falcon-3b`，但由于该模型体积较大，训练时间开销较高，后续改为使用 `TinyLlama` 以提高训练效率。  
- 
- .
    ```
    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
#从transformers库中导入三个接口
#分别用于获取模型，获取模型对应的tokenizer和生成训练参数对象

from datasets import load_dataset
#从Datasets库中导入接口，用于加载并处理原始数据集

from peft import LoraConfig, get_peft_model
#从peft库中导入接口，分别用于创建lora参数对象与构建lora改造版模型

model_name = "tiiuae/falcon-3b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
#与hugging face交互得到模型对应的tokenizer

model = AutoModelForCausalLM.from_pretrained(model_name)
#得到对应的模型

dataset = load_dataset("path_to_your_dataset")
#载入数据集，将原始数据集复制一份，并拆分成train和test两个子集，生成一个DatasetDict对象
#train和test都是Dataset对象，表示一个表格，每一列是一个字段，每一行是一条样本

def format_sample(sample):
    if sample["input"]:
        #如果input值不为空，则需要将input值添加到prompt
        prompt = f"Instruction: {sample['instruction']}\nInput: {sample['input']}\nResponse:"
        #需要保留Instruction，Input这样的关键词的原因是，方便模型识别，划分区域
        #同时在末尾留有response关键词，明确输出开始的位置，显式标记
        #注意这里字典的访问方式，以及python f字符串的写法
    else:
        prompt = f"Instruction: {sample['instruction']}\nResponse:"
    return {
        #注意这里的tokenizer函数，返回一个字典，我们通过["input_ids"]来访问对应的部分
        #下面同理，注意这里的截断是为了确保不会因为输入太长而报错，而补齐是为了更好地拼
        #接成batch，高效计算
        "input_ids": tokenizer(prompt, truncation=True, padding="max_length", max_length=512)["input_ids"],
        "labels": tokenizer(sample["output"], truncation=True, padding="max_length", max_length=512)["input_ids"]
    }

tokenized_dataset = dataset.map(format_sample, remove_columns=dataset["train"].column_names)

#对dataset中的每一个原始数据做同样的操作，包括train和test。同时将旧dataset复制一份，先向复印件添加新的字段与新字段对应的数据。再删除旧字段与旧字段对应的数据
#map函数会完成添加的操作
#这里可以通过列名来删除整个列，包括列名和列数据，因为Dataset按列存储
#这里不用写remove_columns=dataset["test"].column_names是因为train和test字段匹配

lora_config = LoraConfig(
    r=8, #LoRA矩阵的秩，大小决定了矩阵能表达的变化的复杂程度，越大参数越多，时间可能越久
    lora_alpha=32, #LoRA矩阵影响力的缩放系数
    target_modules=["q_proj", "v_proj"], #LoRA矩阵注入的目标模块，这里是注意力模块
    lora_dropout=0.1, #随机屏蔽机制，引入噪声防止过拟合
    bias="none",  #不对原始权重中的bias进行适配
    task_type="CAUSAL_LM" #任务类型，一般选择causal lm，针对语言模型
)

model = get_peft_model(model, lora_config)
#输入原始模型与LoRA参数，得到一个LoRA改版的模型，也就是LoRA矩阵注入后的模型，用于后续训练

#训练参数
training_args = TrainingArguments(
    output_dir="./falcon-3b-lora", #输出目录
    per_device_train_batch_size=4, #每个gpu上的batch大小
    gradient_accumulation_steps=8, #梯度累计，累积几个batch之后才进行一次权重更新
    learning_rate=2e-4, #初始学习率，影响模型学习速度与稳定性
    num_train_epochs=3, #总训练轮数
    warmup_steps=100, #热身步数，前几步线性增加学习率，防止学习率太大导致不稳定
    logging_dir="./logs", #日志导出目录
    logging_steps=50, #每几步记录一次日志
    save_strategy="epoch", #每个epoch后保留模型权重
    evaluation_strategy="epoch", #每个epoch结束时进行一次验证
    fp16=True, #16位浮点数训练，混合精度，加速同时降低显存消耗
    gradient_checkpointing=True, #梯度检查点，在训练时不保存所有中间激活值，用计算换显存占用
    seed=42,
    load_best_model_at_end=True #加载效果最好的模型作为最终模型
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"], #用DatasetDict中的train来训练
    eval_dataset=tokenized_dataset["test"], #用test集来验证
)

trainer.train()
model.save_pretrained("./falcon-3b-lora-final")

    ```
- 
