
作者：卢满江 
学号：202430551043 
班级：24软件工程1班  
日期：2025年8月8日

## 一、项目背景与目标
随着大规模语言模型在自然语言处理领域的广泛应用，微调预训练模型已成为提升模型在特定任务上表现的重要手段。作为机器学习领域的初学者，本项目旨在通过复现基于 LoRA 技术的微调方法，理解和掌握模型微调的基本流程与关键技术。

项目主要目标包括：
- 熟悉 Hugging Face 框架及相关训练工具的使用；
- 理解 LoRA 在模型微调中的原理和作用；
- 掌握数据预处理、模型训练、评估及调参的全过程；
- 在有限的硬件资源下，尝试训练并验证模型效果。
    
通过本项目，期望打下扎实的微调基础，为未来参与更复杂的自然语言处理研究与工程实践积累经验。

## 二、实验环境
- 平台：Kaggle 和 Colab
- GPU：Tesla T4 / 15GB
- Python版本：3.11
- 库依赖：transformers 、peft 、datasets等

## 三、数据集说明
- 原始数据集：Alpaca
- 预处理：取前2600个样本，以调整训练时间

## 四、模型与配置 
- Base模型：TinyLlama/TinyLlama-1.1B-Chat-v1.0
- LoRA参数配置```
```lora_config = LoraConfig
	r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
```
- 训练参数(首轮)```
```training_args = TrainingArguments(
    output_dir="/content/tinyllama-lora",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=3,
    warmup_steps=100,
    logging_dir="./logs",
    logging_steps=50,
    save_strategy="epoch",
    fp16=True,
    gradient_checkpointing=True,
```

## 五、训练记录与结果

| 轮次  | Learning Rate | Batch size * Grad Accum | Epochs | Warmup Steps |
| --- | ------------- | ----------------------- | ------ | ------------ |
| 1   | 2e - 4        | 4 * 8                   | 3      | 100          |
| 2   | 5e - 5        | 8 * 4                   | 4      | 100          |
| 3   | 2e - 5        | 8 * 4                   | 5      | 300          |
| 4   | 5e - 5        | 8 * 4                   | 3      | 50           |
| 5   | 3e - 5        | 8 * 4                   | 3      | 100          |

| 轮次  | 训练时长   | Eval Loss | Perplexity |
| --- | ------ | --------- | ---------- |
| 1   | 7m1s   | 1.5667    | 4.79       |
| 2   | 8m54s  | 1.1518    | 3.16       |
| 3   | 11m23s | 1.1614    | 3.19       |
| 4   | 6m39s  | 1.2872    | 3.62       |
| 5   | 6m45s  | 1.2167    | 3.38       |

## 六、调参分析

第一次调节：
	在初始训练中，GPU 资源占用较低，训练损失和困惑度表现一般。因此，选择增大每台设备上的 batch size，以更充分地利用显存资源、提升训练吞吐量。同时，将梯度累积步数降低为 4，减少优化器的更新间隔，加快训练响应速度。
	此外，考虑到初始阶段模型尚未出现过拟合，适当降低学习率，同时增加训练轮数，以提升整体学习质量。热身步数保持不变，主要用于观察整体训练趋势。

第二次调节：
	第一轮训练后，GPU 利用率有所提升，损失与困惑度均明显下降，模型开始收敛。在此基础上，进一步降低学习率，避免训练过程中的震荡问题。同时，增加训练轮数，以弥补低学习率可能带来的收敛速度变慢。
	考虑到训练初期的稳定性，适当增加热身步数，缓解学习率上升过快带来的训练波动。

第三次调节：
	第三轮训练中，虽然 GPU 占用进一步提升，但损失和困惑度反而略有上升，可能是模型已接近当前配置下的局部最优，或者学习率设置过低，优化停滞。
	为此，适当提升学习率，尝试跳出当前收敛状态。同时，减少热身步数，将更多训练资源集中在主训练阶段。训练轮数略作下降，以控制时间成本。
	
第四次调节：
	第四轮训练结果不理想，损失和困惑度升高，推测可能是学习率提升过快导致模型跳过最优区域。
	因此，本轮调参回归稳健策略，适当降低学习率并提高热身步数，使模型训练更加平稳。训练轮数保持不变，观察是否能够重新收敛。
	根据第五轮训练结果，损失和困惑度重新降低，调参效果良好。

## 七、模型效果评估
- 在五轮训练过程中，模型的损失和困惑度总体呈下降趋势，表明模型在逐步学习并收敛。具体表现如下：
- 第一轮训练损失为 **1.5667**，困惑度为 **4.79**，属于起步阶段；
- 第二轮至第三轮下降明显，最低困惑度降至 **3.16**，训练效果显著；
- 第四轮困惑度略有上升至 **3.62**，可能与参数设置过激有关；
- 第五轮训练后困惑度回落至 **3.38**，说明模型仍具有进一步优化空间。
- 
- 结合各轮的训练日志与评估数据，可以看出模型在调参引导下逐步趋于稳定，收敛速度与资源利用之间达成了初步平衡。

## 八、总结与反思
### 1.实验成果

本次实验较好的完成了预期的任务，使用了LoRA参数高效地进行了训练。通过多轮调参与实验，模型的Loss和Perplexity总体呈现下降趋势，在一定程度上验证了当前训练策略的有效性。具体成果包括：
- 了解并熟悉了基于LoRA的微调训练流程；
- 在有限资源下达到收敛，并成功保存了微调之后的模型；
该实验为后续更大规模模型的训练与下游任务微调提供了良好的起点。

### 2.遇到的问题与解决办法

在训练的过程中遇到了一些典型的问题，在AI的帮助下都得到了解决
- **资源利用不足** ：初期GPU占用较低，训练速度较慢。通过调参的方式，进一步合理分配显存。
- **收敛不稳定** ：Loss与Perplexity有些许波动。 通过调参的方式，调整学习率与热身步数，充分控制训练节奏以提升收敛的稳定性。
- **检查点未保存** ：在某轮训练时曾经遇到过训练结束后未生成checkpoint的情况，通过修改save_strategy和训练轮数的方式，确保保存
- **模型效果波动** ：某一轮Perplexity有反弹的迹象。解决办法是追踪调参逻辑，避免过大调整造成的震荡

### 3.后续计划

本次实验虽取得阶段性成果，但在数据规模、模型效果和评估方式等方面仍有改进空间。后续拟从以下几方面继续推进：

- **探索不同的LoRA参数配置**：如调整rank、alpha值等，以进一步提高微调效率与性能；
- **更换或扩展训练数据集**：在本次实验中，受限于 Kaggle 和 Colab 平台的硬件资源，若使用完整的 Alpaca 数据集，每轮训练耗时长达 27 小时，因此选取了部分子集进行训练，并计划后续通过扩展数据集或更换任务类型进一步提升训练效果。
- **增加评估维度**：尝试 BLEU、ROUGE 或人类评价等方式更全面评估模型效果；
- **考虑多轮对话能力训练**，进一步向对话系统方向靠近。

通过以上改进，期望在后续阶段能训练出更具实用价值的语言模型，同时进一步巩固对微调原理与流程的理解。

### 4.个人反思

- 作为一名刚入门的学习者，由于尚未系统掌握机器学习的基础知识，本次 LoRA 微调任务虽已顺利完成，但理解仍较为浅显。在训练过程中，我主要借助 AI 工具生成代码，并结合 AI 的分析与提示，尽可能理解每一行代码的功能与原理，逐步建立起对训练流程的基本认识。虽然我已经初步掌握了 LoRA 方法及主要训练参数的作用，但对它们之间更深层次的配合关系仍不够熟悉，调参过程较多依赖 AI 的建议与基础直觉，缺乏扎实的理论依据。未来，我计划在夯实机器学习基础知识的前提下，深入研究参数间的联动机制，提升自主分析与调优能力。同时，也将积极熟悉远程算力平台的使用，学习 Linux 系统环境的相关技术，逐步摆脱对免费平台的资源依赖，为后续更大规模、更深入的实验打下基础。

